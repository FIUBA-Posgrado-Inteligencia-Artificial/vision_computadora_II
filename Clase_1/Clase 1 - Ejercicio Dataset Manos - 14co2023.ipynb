{"cells":[{"cell_type":"markdown","metadata":{"id":"NJWHjAloWHNM"},"source":["# Ejercicio Data Augmentation\n","\n","En este ejercicio se utilizará el dataset de manos, en el cual hay imágenes de 64x64 pixeles a color divididas en 6 clases, cada una representando un número distinto (0, 1, 2, 3, 4 o 5) indicado con los dedos de la mano.\n","\n","El conjunto de entrenamiento fue modificado de forma tal que el mismo sobreentrene una red neuronal convolucional si no se aplica ningún tipo de data augmentation. Dentro de la carpeta `train/` se encuentran 6 carpetas, cada una con 30 imágenes. El conjunto de validación no fue modificado. El mismo se encuentra en la carpeta `valid/` con 20 imágenes en cada clase.\n","\n","El objetivo de este ejercicio es, entonces, realizar entrenamientos de una misma arquitectura de red convolucional, utilizando data augmentation para obtener la mejor performance posible."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oSDFk1ctX20g"},"outputs":[],"source":["!pip install torchmetrics\n","!pip install --upgrade --no-cache-dir gdown"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QCxrrCdVSKet"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import random\n","import os\n","\n","import torch\n","import torchvision\n","import torchsummary\n","import torchmetrics\n","from torch.utils.tensorboard import SummaryWriter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m4S2_NugX20j"},"outputs":[],"source":["!gdown 1e0bdIt_uCaTof7i57-O7XZ-wPy15nikT\n","!unzip dataset_manos_modificado.zip > /dev/null"]},{"cell_type":"markdown","metadata":{"id":"T97WZY-xoLLT"},"source":["### Visualización de los datos\n","\n","Con la siguiente celda de código podemos visualizar una imagen de cada clase, tomada al azar, del conjunto de entrenamiento. Siempre es importante tener nocion de como lucen las imágenes de nuestro set de datos, tanto de entrenamiento como validación. Esto nos ayudará a saber que tipos de transformaciones serán más útiles cuando realicemos data augmentation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lQ1I4IJSnV6h"},"outputs":[],"source":["train_folder = './dataset_manos_modificado/train/'\n","\n","fig, axs = plt.subplots(1, 6, figsize=(20, 20))\n","\n","for id, class_folder in enumerate(sorted(os.listdir(train_folder))):\n","\n","    image_name = random.choice(os.listdir(os.path.join(train_folder, class_folder)))\n","    image = mpimg.imread(os.path.join(train_folder, class_folder, image_name))\n","\n","    axs[id].imshow(image)\n","    axs[id].set_title(class_folder)"]},{"cell_type":"markdown","metadata":{"id":"oOOjs1ZmX20m"},"source":["### Definimos nuestra función de entrenamiento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"icZqJ2wNX20n"},"outputs":[],"source":["def train(model, optimizer, criterion, metric, data, epochs, tb_writer=None):\n","\n","    train_loader = data[\"train\"]\n","    valid_loader = data[\"valid\"]\n","\n","    train_writer = tb_writer[\"train\"]\n","    valid_writer = tb_writer[\"valid\"]\n","\n","    if tb_writer:\n","        train_writer.add_graph(model, torch.zeros((1, 3, data[\"image_width\"], data[\"image_height\"])))\n","        valid_writer.add_graph(model, torch.zeros((1, 3, data[\"image_width\"], data[\"image_height\"])))\n","\n","    if torch.cuda.is_available():\n","        model.to(\"cuda\")\n","        metric.to(\"cuda\")\n","\n","    train_loss = []\n","    train_acc = []\n","    valid_loss = []\n","    valid_acc = []\n","\n","    for epoch in range(epochs):\n","\n","        # Pongo el modelo en modo entrenamiento\n","        model.train()\n","\n","        epoch_train_loss = 0.0\n","        epoch_train_accuracy = 0.0\n","\n","        for train_data, train_target in train_loader:\n","\n","            if torch.cuda.is_available():\n","                train_data.to(\"cuda\")\n","                train_target.to(\"cuda\")\n","\n","            optimizer.zero_grad()\n","            output = model(train_data.float())\n","            loss = criterion(output, train_target)\n","            epoch_train_loss += loss.item()\n","            loss.backward()\n","            optimizer.step()\n","\n","            accuracy = metric(output, train_target)\n","            epoch_train_accuracy += accuracy.item()\n","\n","        epoch_train_loss = epoch_train_loss / len(train_loader)\n","        epoch_train_accuracy = epoch_train_accuracy / len(train_loader)\n","        train_loss.append(epoch_train_loss)\n","        train_acc.append(epoch_train_accuracy)\n","\n","        # Pongo el modelo en modo testeo\n","        model.eval()\n","\n","        epoch_valid_loss = 0.0\n","        epoch_valid_accuracy = 0.0\n","\n","        for valid_data, valid_target in valid_loader:\n","            if torch.cuda.is_available():\n","                valid_data.to(\"cuda\")\n","                valid_target.to(\"cuda\")\n","\n","            output = model(valid_data.float())\n","            epoch_valid_loss += criterion(output, valid_target).item()\n","            epoch_valid_accuracy += metric(output, valid_target).item()\n","\n","        epoch_valid_loss = epoch_valid_loss / len(valid_loader)\n","        epoch_valid_accuracy = epoch_valid_accuracy / len(valid_loader)\n","        valid_loss.append(epoch_valid_loss)\n","        valid_acc.append(epoch_valid_accuracy)\n","\n","        print(\"Epoch: {}/{} - Train loss {:.6f} - Train Accuracy {:.6f} - Valid Loss {:.6f} - Valid Accuracy {:.6f}\".format(\n","        epoch+1, epochs, epoch_train_loss, epoch_train_accuracy, epoch_valid_loss, epoch_valid_accuracy))\n","\n","        if tb_writer:\n","            train_writer.add_scalar(\"loss\", epoch_train_loss, epoch)\n","            valid_writer.add_scalar(\"loss\", epoch_valid_loss, epoch)\n","            train_writer.add_scalar(\"accuracy\", epoch_train_accuracy, epoch)\n","            valid_writer.add_scalar(\"accuracy\", epoch_valid_accuracy, epoch)\n","            train_writer.flush()\n","            valid_writer.flush()\n","\n","    history = {}\n","    history[\"train_loss\"] = train_loss\n","    history[\"train_acc\"] = train_acc\n","    history[\"valid_loss\"] = valid_loss\n","    history[\"valid_acc\"] = valid_acc\n","\n","    return history"]},{"cell_type":"markdown","metadata":{"id":"Ft3QTJ7aX20p"},"source":["### Definimos nuestra arquitectura de red neuronal\n","\n","En la siguiente celda se define una clase para una red neuronal convolucional con 4 capas convolucionales, cada una seguida de un max pooling. Luego del último bloque convolucional-pooling se realiza un flatten y se agregan 2 capas densas para la clasificación.\n","\n","Para que esta celda funcione se debe completar el valor de `in_features` de la primera capa densa, el cual se puede calcular sabiendo el tamaño de las imágenes de entrada a la red, y cómo afecta cada capa convolucional y de pooling a las dimensión del tensor de entrada de dicha capa densa."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JnU3wRWgX20q"},"outputs":[],"source":["CANTIDAD_CLASES = 6\n","ANCHO_IMAGENES = 64\n","ALTO_IMAGENES = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GeQE6EMbX20q"},"outputs":[],"source":["class ConvModel(torch.nn.Module):\n","    def __init__(self, output_units):\n","        super().__init__()\n","        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding='same')\n","        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding='same')\n","        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv3 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same')\n","        self.pool3 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv4 = torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding='same')\n","        self.pool4 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.fc1 = torch.nn.Linear(in_features=None, out_features=512)\n","        self.fc2 = torch.nn.Linear(in_features=512, out_features=output_units)\n","\n","    def forward(self, x):\n","        x = self.pool1(torch.relu(self.conv1(x)))\n","        x = self.pool2(torch.relu(self.conv2(x)))\n","        x = self.pool3(torch.relu(self.conv3(x)))\n","        x = self.pool4(torch.relu(self.conv4(x)))\n","        x = torch.flatten(x, 1)\n","        x = torch.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","conv_model = ConvModel(CANTIDAD_CLASES)\n","\n","# Si hay una GPU disponible muevo el modelo allí para aprovechar ese recurso\n","if torch.cuda.is_available():\n","    conv_model.to(\"cuda\")\n","\n","torchsummary.summary(conv_model, (3, ANCHO_IMAGENES, ALTO_IMAGENES))"]},{"cell_type":"markdown","metadata":{"id":"m8fegZFilttF"},"source":["### 1) Entrenar sin aplicar Data Augmentation\n","\n","En primer lugar se define una composición de transformaciones que no realice ninguna modificación sobre las imágenes más que pasarlas a tensores, como lo pide torchvision. Luego se definen los `DataLoaders` para el entrenamiento."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dL2JlRBBVVjS"},"outputs":[],"source":["data_transforms = torchvision.transforms.Compose([\n","                    torchvision.transforms.ToTensor()\n","                  ])\n","\n","train_set = torchvision.datasets.ImageFolder(root='./dataset_manos_modificado/train', transform=data_transforms)\n","valid_set = torchvision.datasets.ImageFolder(root='./dataset_manos_modificado/valid', transform=data_transforms)\n","\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n","valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=32, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"h4DwLithX20s"},"source":["Realizar el entrenamiento de la red durante 30 épocas y observar el sobreentrenamiento que se genera en el modelo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"04-E2DGlX20s"},"outputs":[],"source":["noaug_conv_model = ConvModel(CANTIDAD_CLASES)\n","noaug_optimizer = torch.optim.Adam(noaug_conv_model.parameters(), lr=0.001)\n","noaug_loss = torch.nn.CrossEntropyLoss()\n","noaug_metric = torchmetrics.Accuracy(task='multiclass', num_classes=CANTIDAD_CLASES)\n","noaug_data = {\"train\": train_loader, \"valid\": valid_loader, \"image_width\": ANCHO_IMAGENES, \"image_height\": ALTO_IMAGENES}\n","\n","noaug_writer = {\"train\": SummaryWriter(log_dir=\"data_aug/noaug_train\"),\n","                \"valid\": SummaryWriter(log_dir=\"data_aug/noaug_valid\")}\n","\n","history = train(noaug_conv_model,\n","                noaug_optimizer,\n","                noaug_loss,\n","                noaug_metric,\n","                noaug_data,\n","                30,\n","                noaug_writer)\n","\n","fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n","\n","axs[0].plot(history[\"train_loss\"])\n","axs[0].plot(history[\"valid_loss\"])\n","axs[0].title.set_text('Error de Entrenamiento vs Validación')\n","axs[0].legend(['Train', 'Valid'])\n","\n","axs[1].plot(history[\"train_acc\"])\n","axs[1].plot(history[\"valid_acc\"])\n","axs[1].title.set_text('Accuracy de Entrenamiento vs Validación')\n","axs[1].legend(['Train', 'Valid'])"]},{"cell_type":"markdown","metadata":{"id":"5xrQcjrmARne"},"source":["---\n","### 2) Entrenar aplicando Data Augmentation\n","\n","A continuación se define una composición de transformaciones para aplicar data augmentation sobre el conjunto de entrenamiento.\n","\n","Ejecutar la celda para entrenar durante, al menos, 100 épocas y analizar los resultados."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GggdvCPBX20t"},"outputs":[],"source":["aug_data_transforms = torchvision.transforms.Compose([\n","                        torchvision.transforms.RandomHorizontalFlip(0.5),\n","                        torchvision.transforms.RandomResizedCrop(size=(64, 64), scale=(0.2, 0.7)),\n","                        torchvision.transforms.RandomRotation(degrees=90),\n","                        torchvision.transforms.RandomPerspective(0.8),\n","                        torchvision.transforms.RandomGrayscale(0.5),\n","                        torchvision.transforms.ToTensor(),\n","                      ])\n","\n","aug_train_set = torchvision.datasets.ImageFolder(root='./dataset_manos_modificado/train', transform=aug_data_transforms)\n","\n","aug_train_loader = torch.utils.data.DataLoader(aug_train_set, batch_size=32, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZrRaHXOgX20u"},"outputs":[],"source":["aug_conv_model = ConvModel(CANTIDAD_CLASES)\n","aug_optimizer = torch.optim.Adam(aug_conv_model.parameters(), lr=0.001)\n","aug_loss = torch.nn.CrossEntropyLoss()\n","aug_metric = torchmetrics.Accuracy(task='multiclass', num_classes=CANTIDAD_CLASES)\n","aug_data = {\"train\": aug_train_loader, \"valid\": valid_loader, \"image_width\": ANCHO_IMAGENES, \"image_height\": ALTO_IMAGENES}\n","\n","aug_writer = {\"train\": SummaryWriter(log_dir=\"data_aug/aug_train\"),\n","              \"valid\": SummaryWriter(log_dir=\"data_aug/aug_valid\")}\n","\n","history = train(aug_conv_model,\n","                aug_optimizer,\n","                aug_loss,\n","                aug_metric,\n","                aug_data,\n","                100,\n","                aug_writer)\n","\n","fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n","\n","axs[0].plot(history[\"train_loss\"])\n","axs[0].plot(history[\"valid_loss\"])\n","axs[0].title.set_text('Error de Entrenamiento vs Validación')\n","axs[0].legend(['Train', 'Valid'])\n","\n","axs[1].plot(history[\"train_acc\"])\n","axs[1].plot(history[\"valid_acc\"])\n","axs[1].title.set_text('Accuracy de Entrenamiento vs Validación')\n","axs[1].legend(['Train', 'Valid'])"]},{"cell_type":"markdown","source":["###**Algunas preguntas para analizar**\n","\n","1) ¿Es el resultado del entrenamiento aceptable?\n","\n","2) ¿Está el Data Augmentation mejorando o empeorando el resultado?\n","\n","3) Basándose en las características de las imágenes del conjunto de validación, diría que el tipo de transformaciones utilizadas es acertado? (Se puede usar la celda del principio del Colab para visualizar imágenes del conjunto de validación)\n","\n","4) Basándose en los resultados del entrenamiento, diría que los parámetros de las transformaciones aplicadas tienen valores agresivos? (Se puede utilizar la celda siguiente para visualizar un conjunto de imágenes y sus versiones transformadas)"],"metadata":{"id":"7oUrn94buhL1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mMzH_oqQf1wD"},"outputs":[],"source":["# Ploteo las imagenes sin augmentacion\n","\n","images_ids = np.random.randint(low=0, high=len(train_set), size=4)\n","\n","fig, rows = plt.subplots(nrows=1, ncols=4, figsize=(18, 18))\n","for id, row in enumerate(rows):\n","    row.imshow(train_set[images_ids[id]][0].permute(1, 2, 0))\n","    row.axis('off')\n","plt.show()\n","\n","# Ploteo las mismas imagenes con augmentacion\n","\n","fig, rows = plt.subplots(nrows=1, ncols=4, figsize=(18, 18))\n","for id, row in enumerate(rows):\n","    row.imshow(aug_train_set[images_ids[id]][0].permute(1, 2, 0))\n","    row.axis('off')\n","plt.show()"]},{"cell_type":"markdown","source":["### 3) Mejorar/corregir el Data Augmentation\n","\n","En base a su criterio y a las reflexiones de las preguntas anteriores, defina una nueva composición de transformaciones que ayude a obtener buenos resultados sobre el entrenamiento. Intentar obtener métricas que superen el 90% de accuracy sin caer en sobreentrenamiento. Incrementar la cantidad de épocas de entrenamiento si es necesario."],"metadata":{"id":"prY9RgzEwBlp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"e1Wv3Ozhd2DC"},"outputs":[],"source":["new_aug_data_transforms = torchvision.transforms.Compose([\n","                            ### COMPLETAR CON TRANSFORMACIONES\n","                            torchvision.transforms.ToTensor(),\n","                        ])\n","\n","new_aug_train_set = torchvision.datasets.ImageFolder(root='./dataset_manos_modificado/train', transform=new_aug_data_transforms)\n","\n","new_aug_train_loader = torch.utils.data.DataLoader(new_aug_train_set, batch_size=32, shuffle=True)"]},{"cell_type":"code","source":["new_aug_conv_model = ConvModel(CANTIDAD_CLASES)\n","new_aug_optimizer = torch.optim.Adam(new_aug_conv_model.parameters(), lr=0.001)\n","new_aug_loss = torch.nn.CrossEntropyLoss()\n","new_aug_metric = torchmetrics.Accuracy(task='multiclass', num_classes=CANTIDAD_CLASES)\n","new_aug_data = {\"train\": new_aug_train_loader, \"valid\": valid_loader, \"image_width\": ANCHO_IMAGENES, \"image_height\": ALTO_IMAGENES}\n","\n","new_aug_writer = {\"train\": SummaryWriter(log_dir=\"data_aug/new_aug_train\"),\n","                  \"valid\": SummaryWriter(log_dir=\"data_aug/new_aug_valid\")}\n","\n","history = train(new_aug_conv_model,\n","                new_aug_optimizer,\n","                new_aug_loss,\n","                new_aug_metric,\n","                new_aug_data,\n","                100,\n","                new_aug_writer)\n","\n","fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n","\n","axs[0].plot(history[\"train_loss\"])\n","axs[0].plot(history[\"valid_loss\"])\n","axs[0].title.set_text('Error de Entrenamiento vs Validación')\n","axs[0].legend(['Train', 'Valid'])\n","\n","axs[1].plot(history[\"train_acc\"])\n","axs[1].plot(history[\"valid_acc\"])\n","axs[1].title.set_text('Accuracy de Entrenamiento vs Validación')\n","axs[1].legend(['Train', 'Valid'])"],"metadata":{"id":"gFdMN_NaxBVl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Conclusiones\n","\n","- Que tipo de transformaciones son mas efectivas en este dataset?\n"],"metadata":{"id":"r6nR_WBD5u9e"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"ceia-2qPzDgTX","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"6a7e264b0185234a6d090a42ff6d01e38825180e16d17b7c5c45b22f8dc26b79"}}},"nbformat":4,"nbformat_minor":0}